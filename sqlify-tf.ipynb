{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./processed/final_data.csv\")\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((df[\"question_header\"], df[\"sql\"]))\n",
    "\n",
    "train_size = int(0.8 * len(df))\n",
    "train_dataset = train_dataset.take(train_size)\n",
    "val_dataset = train_dataset.skip(train_size)\n",
    "\n",
    "batch_size = 32\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_size).batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "type(train_dataset)\n",
    "questions = \"\"\n",
    "SQL = \"\"\n",
    "\n",
    "# train_dataset.tak\n",
    "for question, sql in train_dataset.take(1):\n",
    "    questions = question.numpy()\n",
    "    SQL = sql.numpy()\n",
    "    # print(\"Question:\", question.numpy())\n",
    "    # print(\"SQL:\", sql.numpy())\n",
    "    # break\n",
    "questions.shape\n",
    "SQL.shape\n",
    "# prints a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs for first SQL query: [4, 200, 3, 302, 9405, 1, 256, 714]\n",
      "Token IDs for first question: [253, 247, 8, 2, 200, 110, 14, 256, 714, 3595, 13244, 1555, 190, 302, 9405, 302, 63, 200]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Concatenate SQL queries and natural language questions\n",
    "texts = df['sql'].tolist() + df['question_header'].tolist()\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(filters='')\n",
    "\n",
    "# Fit tokenizer on texts\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences of token IDs\n",
    "sequences_sql = tokenizer.texts_to_sequences(df['sql'])\n",
    "sequences_questions = tokenizer.texts_to_sequences(df['question_header'])\n",
    "\n",
    "# Example usage of token IDs\n",
    "print(\"Token IDs for first SQL query:\", sequences_sql[0])\n",
    "print(\"Token IDs for first question:\", sequences_questions[0])\n",
    "\n",
    "# tokenizer.save('sql_tokenizer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 200, 3, 302, 9405, 1, 256, 714]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences_sql)\n",
    "sequences_sql[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your preprocessed dataset\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"./processed/final_data.csv\")\n",
    "\n",
    "\n",
    "train_data, val_data = train_test_split(dataset, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the English sentences\n",
    "tokenizer_en = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer_en.fit_on_texts(train_data['question_header'])\n",
    "train_inputs = tokenizer_en.texts_to_sequences(train_data['question_header'])\n",
    "val_inputs = tokenizer_en.texts_to_sequences(val_data['question_header'])\n",
    "\n",
    "# Tokenize the SQL queries\n",
    "tokenizer_sql = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer_sql.fit_on_texts(train_data['sql'])\n",
    "train_outputs = tokenizer_sql.texts_to_sequences(train_data['sql'])\n",
    "val_outputs = tokenizer_sql.texts_to_sequences(val_data['sql'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad sequences to the same length\n",
    "max_length = max(len(seq) for seq in train_inputs + val_inputs)\n",
    "train_inputs = pad_sequences(train_inputs, maxlen=max_length, padding='post')\n",
    "val_inputs = pad_sequences(val_inputs, maxlen=max_length, padding='post')\n",
    "train_outputs = pad_sequences(train_outputs, maxlen=max_length, padding='post')\n",
    "val_outputs = pad_sequences(val_outputs, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_inputs, train_outputs)).shuffle(len(train_data)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_inputs, val_outputs)).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads, d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.mha(inputs, inputs, inputs)  # Self-attention\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layer_norm1(inputs + attn_output)  # Residual connection and layer normalization\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # Feed-forward network\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layer_norm2(out1 + ffn_output)  # Residual connection and layer normalization\n",
    "\n",
    "        return out2\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.num_layers = num_layers\n",
    "        self.transformer_blocks = [TransformerBlock(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        enc_inputs = inputs[0]\n",
    "        dec_inputs = inputs[1]\n",
    "        \n",
    "        enc_padding_mask = inputs[2]\n",
    "        look_ahead_mask = inputs[3]\n",
    "        dec_padding_mask = inputs[4]\n",
    "        \n",
    "        enc_inputs = enc_inputs + self.positional_encoding(enc_inputs.shape[1], d_model)\n",
    "        dec_inputs = dec_inputs + self.positional_encoding(dec_inputs.shape[1], d_model)\n",
    "\n",
    "        enc_output = self.encoder(enc_inputs)\n",
    "        dec_output = self.decoder(dec_inputs)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            enc_output = self.transformer_blocks[i](enc_output, training=training)\n",
    "            dec_output = self.transformer_blocks[i](dec_output, training=training)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "        return final_output\n",
    "\n",
    "# Function to create positional encodings\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "    # Apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # Apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10 \n",
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "input_vocab_size = len(tokenizer_en.word_index) + 1\n",
    "target_vocab_size = len(tokenizer_sql.word_index) + 1\n",
    "dropout_rate = 0.1\n",
    "input_length = max_length\n",
    "target_length = max_length\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and compile the model\n",
    "transformer = Transformer(num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, input_length, target_length, dropout_rate)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "transformer.compile(optimizer=optimizer, loss=loss_function, run_eagerly=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Exception encountered when calling layer 'softmax_1' (type Softmax).\n\ntuple index out of range\n\nCall arguments received by layer 'softmax_1' (type Softmax):\n  • inputs=tf.Tensor(shape=(114, 8), dtype=float32)\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DISK-W/FYP/Generating-Sql-Quesries-from-User-Questions/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[81], line 52\u001b[0m, in \u001b[0;36mTransformer.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     49\u001b[0m dec_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(dec_inputs)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m---> 52\u001b[0m     enc_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_blocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     dec_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks[i](dec_output, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m     55\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer(dec_output)\n",
      "Cell \u001b[0;32mIn[81], line 20\u001b[0m, in \u001b[0;36mTransformerBlock.call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 20\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Self-attention\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(attn_output, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[1;32m     22\u001b[0m     out1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm1(inputs \u001b[38;5;241m+\u001b[39m attn_output)  \u001b[38;5;66;03m# Residual connection and layer normalization\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: Exception encountered when calling layer 'softmax_1' (type Softmax).\n\ntuple index out of range\n\nCall arguments received by layer 'softmax_1' (type Softmax):\n  • inputs=tf.Tensor(shape=(114, 8), dtype=float32)\n  • mask=None"
     ]
    }
   ],
   "source": [
    "transformer.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
