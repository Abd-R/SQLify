{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-03-20 01:58:12.900352: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-20 01:58:12.963173: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-20 01:58:13.195143: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 01:58:14.147018: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./processed/wiki-sql-2.csv')\n",
    "# data = pd.read_csv('./processed/wiki_sql_full.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "\n",
    "    # def formulate_schema(cols):\n",
    "    #     index = []\n",
    "    #     for idx, col in enumerate(cols):\n",
    "    #         index.append(f'<col{idx + 1}> {col}')\n",
    "    #     return index\n",
    "\n",
    "    def combine_columns(row):\n",
    "        return row['question'] + ' ' + ' '.join(row['table'])\n",
    "    \n",
    "    data['sql'] = data['sql'].apply(lambda x: (ast.literal_eval(x))['human_readable'])\n",
    "    data['table'] = data['table'].apply(lambda x: (ast.literal_eval(x))['header'])\n",
    "    # data['table'] = data['table'].apply(formulate_schema)\n",
    "\n",
    "    data['question_header'] = data.apply(combine_columns, axis=1)\n",
    "    data['question_header'] = data['question_header'].str.replace('/', ' ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tell me what the notes are for South Australia  <col1> State territory <col2> Text background colour <col3> Format <col4> Current slogan <col5> Current series <col6> Notes'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess()\n",
    "data.question_header.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'ted_hrlr_translate_pt_en_converter'\n",
    "# model = tf.saved_model.load(model_path)\n",
    "\n",
    "model_name = 'ted_hrlr_translate_pt_en_converter'\n",
    "# tf.keras.utils.get_file(\n",
    "#     f'{model_name}.zip',\n",
    "#     f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n",
    "#     cache_dir='.', cache_subdir='', extract=True\n",
    "# )\n",
    "\n",
    "tokenizers = tf.saved_model.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_questions = tokenizers.en.tokenize(data['question_header'].tolist())\n",
    "# encoded_sql = tokenizers.en.tokenize(data['sql'].tolist())\n",
    "\n",
    "# decoded_questions = tokenizers.en.detokenize(encoded_questions).numpy()\n",
    "# decoded_sql = tokenizers.en.detokenize(encoded_sql).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(2):\n",
    "#     print(f\"Original Question: {data['question_header'][i]}\")\n",
    "#     print(f\"Tokenized Question: {encoded_questions[i]}\")\n",
    "#     print(f\"Detokenized Question: {decoded_questions[i]}\")\n",
    "\n",
    "#     print(f\"Original SQL: {data['sql'][i]}\")\n",
    "#     print(f\"Tokenized SQL: {encoded_sql[i]}\")\n",
    "#     print(f\"Detokenized SQL: {decoded_sql[i]}\")\n",
    "\n",
    "#     print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question_lengths = encoded_questions.row_lengths().numpy()\n",
    "# sql_lengths = encoded_sql.row_lengths().numpy()\n",
    "\n",
    "# # encoded_questions.row_lengths()\n",
    "\n",
    "# for i in range(5):\n",
    "#     print('Ques:', question_lengths[i],  '- SQL:', sql_lengths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# all_lengths = np.concatenate([question_lengths, sql_lengths])\n",
    "\n",
    "# # Plot the histogram\n",
    "# plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
    "# plt.ylim(plt.ylim())\n",
    "# max_length = max(all_lengths)\n",
    "\n",
    "# # Add a vertical line at the maximum length\n",
    "# plt.plot([max_length, max_length], plt.ylim())\n",
    "# plt.title(f'Maximum tokens per example: {max_length}')\n",
    "# plt.xlabel('Token Count')\n",
    "# plt.ylabel('Number of Examples')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = 128\n",
    "\n",
    "def prepare_batch(questions, sql_queries):\n",
    "    # Tokenize questions and SQL queries\n",
    "    questions_tokenized = tokenizers.en.tokenize(questions)\n",
    "    sql_queries_tokenized = tokenizers.en.tokenize(sql_queries)\n",
    "    \n",
    "    # Trim tokenized sequences to MAX_TOKENS\n",
    "    questions_tokenized = questions_tokenized[:, :MAX_TOKENS]\n",
    "    sql_queries_tokenized = sql_queries_tokenized[:, :(MAX_TOKENS + 1)]  # Shift for labels\n",
    "    \n",
    "    # Convert to dense tensors\n",
    "    questions_inputs = questions_tokenized.to_tensor()\n",
    "    sql_inputs = sql_queries_tokenized[:, :-1].to_tensor()  # Drop the [END] tokens\n",
    "    sql_labels = sql_queries_tokenized[:, 1:].to_tensor()  # Drop the [START] tokens\n",
    "    \n",
    "    return (questions_inputs, sql_inputs), sql_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = 'Tell me what the notes are for South Australia  State territory Text background colour Format Current slogan Current series Notes'\n",
    "# s = 'SELECT Notes FROM table WHERE Current slogan = SOUTH AUSTRALIA'\n",
    "# (q, s), s_label = prepare_batch([q], [s])\n",
    "# print(q)\n",
    "# print(s)\n",
    "# print(s_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Same and shifted by 1\n",
    "# print(s[:13])\n",
    "# print(s_label[:13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(ds):\n",
    "  return (\n",
    "      ds\n",
    "      .shuffle(BUFFER_SIZE)\n",
    "      .batch(BATCH_SIZE)\n",
    "      .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "      .prefetch(buffer_size=tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (64523, 6)\n",
      "Validation set shape: (8065, 6)\n",
      "Test set shape: (8066, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, temp = train_test_split(data, test_size=0.2, random_state=42)\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Train set shape:\", train.shape)\n",
    "print(\"Validation set shape:\", val.shape)\n",
    "print(\"Test set shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_questions_data = train['question_header'].tolist()\n",
    "train_sql_queries_data = train['sql'].tolist()\n",
    "\n",
    "test_questions_data = test['question_header'].tolist()\n",
    "test_sql_queries_data = test['sql'].tolist()\n",
    "\n",
    "val_questions_data = val['question_header'].tolist()\n",
    "val_sql_queries_data = val['sql'].tolist()\n",
    "# Create TensorFlow Datasets\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_questions_data, train_sql_queries_data))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_questions_data, test_sql_queries_data))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_questions_data, val_sql_queries_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = make_batches(train_dataset)\n",
    "test_batches = make_batches(test_dataset)\n",
    "val_batches = make_batches(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 107)\n",
      "(64, 48)\n",
      "(64, 48)\n"
     ]
    }
   ],
   "source": [
    "for (ques, sql), sql_labels in train_batches.take(1):\n",
    "  break\n",
    "\n",
    "print(ques.shape)\n",
    "print(sql.shape)\n",
    "print(sql_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "  depth = depth/2\n",
    "\n",
    "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
    "  \n",
    "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
    "  angle_rads = positions * angle_rates      # (pos, depth)\n",
    "\n",
    "  pos_encoding = np.concatenate(\n",
    "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "      axis=-1) \n",
    "\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #@title\n",
    "# pos_encoding = positional_encoding(length=2048, depth=512)\n",
    "\n",
    "# # Check the shape.\n",
    "# print(pos_encoding.shape)\n",
    "\n",
    "# # Plot the dimensions.\n",
    "# plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "# plt.ylabel('Depth')\n",
    "# plt.xlabel('Position')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, vocab_size, d_model):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero=True) \n",
    "    self.pos_encoding = positional_encoding(length=2048, depth=d_model)\n",
    "\n",
    "  def compute_mask(self, *args, **kwargs):\n",
    "    return self.embedding.compute_mask(*args, **kwargs)\n",
    "\n",
    "  def call(self, x):\n",
    "    length = tf.shape(x)[1]\n",
    "    x = tf.cast(x, tf.int32)\n",
    "\n",
    "    x = self.embedding(x)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VocabSize = tokenizers.en.get_vocab_size()\n",
    "# print(VocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_en = PositionalEmbedding(vocab_size=tokenizers.en.get_vocab_size(), d_model=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ques = tf.cast(ques, dtype=tf.int32)\n",
    "# embed_en(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_en(ques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__()\n",
    "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    self.add = tf.keras.layers.Add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossAttention(BaseAttention):\n",
    "  def call(self, x, context):\n",
    "    attn_output, attn_scores = self.mha(\n",
    "        query=x,\n",
    "        key=context,\n",
    "        value=context,\n",
    "        return_attention_scores=True)\n",
    "   \n",
    "    # Cache the attention scores for plotting later.\n",
    "    self.last_attn_scores = attn_scores\n",
    "\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(BaseAttention):\n",
    "  def call(self, x):\n",
    "    attn_output = self.mha(\n",
    "        query=x,\n",
    "        value=x,\n",
    "        key=x,\n",
    "        use_causal_mask = True)\n",
    "    x = self.add([x, attn_output])\n",
    "    x = self.layernorm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),\n",
    "      tf.keras.layers.Dense(d_model),\n",
    "      tf.keras.layers.Dropout(dropout_rate)\n",
    "    ])\n",
    "    self.add = tf.keras.layers.Add()\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.add([x, self.seq(x)])\n",
    "    x = self.layer_norm(x) \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.self_attention = GlobalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.self_attention(x)\n",
    "    x = self.ffn(x)\n",
    "    return x\n",
    "  \n",
    "  # def get_config(self):\n",
    "  #   config = super().get_config().copy()\n",
    "  #   config.update({\n",
    "  #       'd_model': self.d_model,\n",
    "  #       'num_heads': self.num_heads,\n",
    "  #       'dff': self.dff,\n",
    "  #       \"dropout_rate\": self.dropout_rate\n",
    "  #   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads,\n",
    "               dff, vocab_size, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(\n",
    "        vocab_size=vocab_size, d_model=d_model)\n",
    "\n",
    "    self.enc_layers = [\n",
    "        EncoderLayer(d_model=d_model,\n",
    "                     num_heads=num_heads,\n",
    "                     dff=dff,\n",
    "                     dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "  def call(self, x):\n",
    "    # `x` is token-IDs shape: (batch, seq_len)\n",
    "    x = self.pos_embedding(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    \n",
    "    # Add dropout.\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x = self.enc_layers[i](x)\n",
    "\n",
    "    return x  # Shape `(batch_size, seq_len, d_model)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               d_model,\n",
    "               num_heads,\n",
    "               dff,\n",
    "               dropout_rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.causal_self_attention = CausalSelfAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "    \n",
    "    self.cross_attention = CrossAttention(\n",
    "        num_heads=num_heads,\n",
    "        key_dim=d_model,\n",
    "        dropout=dropout_rate)\n",
    "\n",
    "    self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "  def call(self, x, context):\n",
    "    x = self.causal_self_attention(x=x)\n",
    "    x = self.cross_attention(x=x, context=context)\n",
    "\n",
    "    # Cache the last attention scores for plotting later\n",
    "    self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "    x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    return x\n",
    "  \n",
    "  # def get_config(self):\n",
    "  #   config = super().get_config().copy()\n",
    "  #   config.update({\n",
    "  #       #'vocab_size': self.vocab_size,\n",
    "  #       #'num_layers': self.num_layers,\n",
    "  #       #'units': self.units,\n",
    "  #       'd_model': self.d_model,\n",
    "  #       'num_heads': self.num_heads,\n",
    "  #       'dff': self.dff,\n",
    "  #       \"dropout_rate\": self.dropout_rate\n",
    "  #   })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size,\n",
    "               dropout_rate=0.1):\n",
    "    super(Decoder, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.num_layers = num_layers\n",
    "\n",
    "    self.pos_embedding = PositionalEmbedding(vocab_size=vocab_size,\n",
    "                                             d_model=d_model)\n",
    "    self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    self.dec_layers = [\n",
    "        DecoderLayer(d_model=d_model, num_heads=num_heads,\n",
    "                     dff=dff, dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "\n",
    "    self.last_attn_scores = None\n",
    "\n",
    "  def call(self, x, context):\n",
    "    # `x` is token-IDs shape (batch, target_seq_len)\n",
    "    x = self.pos_embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "    x = self.dropout(x)\n",
    "\n",
    "    for i in range(self.num_layers):\n",
    "      x  = self.dec_layers[i](x, context)\n",
    "\n",
    "    self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "\n",
    "    # The shape of x is (batch_size, target_seq_len, d_model).\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "@keras.utils.register_keras_serializable()\n",
    "class Transformer(tf.keras.Model):\n",
    "  def __init__(self, *, num_layers, d_model, num_heads, dff,\n",
    "               input_vocab_size, target_vocab_size, dropout_rate=0.1, **kwargs):\n",
    "    super().__init__()\n",
    "    self.num_layers = num_layers\n",
    "    self.d_model = d_model\n",
    "    self.num_heads = num_heads\n",
    "    self.dff = dff\n",
    "    self.input_vocab_size = input_vocab_size\n",
    "    self.target_vocab_size = target_vocab_size\n",
    "    self.dropout_rate = dropout_rate\n",
    "        \n",
    "    self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=input_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n",
    "                           num_heads=num_heads, dff=dff,\n",
    "                           vocab_size=target_vocab_size,\n",
    "                           dropout_rate=dropout_rate)\n",
    "\n",
    "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # To use a Keras model with `.fit` you must pass all your inputs in the\n",
    "    # first argument.\n",
    "    context, x  = inputs\n",
    "\n",
    "    context = self.encoder(context)  # (batch_size, context_len, d_model)\n",
    "\n",
    "    x = self.decoder(x, context)  # (batch_size, target_len, d_model)\n",
    "\n",
    "    # Final linear layer output.\n",
    "    logits = self.final_layer(x)  # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "    try:\n",
    "      # Drop the keras mask, so it doesn't scale the losses/metrics.\n",
    "      # b/250038731\n",
    "      del logits._keras_mask\n",
    "    except AttributeError:\n",
    "      pass\n",
    "\n",
    "    # Return the final output and the attention weights.\n",
    "    return logits\n",
    "  \n",
    "  def get_config(self):\n",
    "    config = super().get_config().copy()\n",
    "    config.update({\n",
    "        'input_vocab_size': self.input_vocab_size,\n",
    "        'target_vocab_size': self.target_vocab_size,\n",
    "        'num_layers': self.num_layers,\n",
    "        'd_model': self.d_model,\n",
    "        'num_heads': self.num_heads,\n",
    "        'dff': self.dff,\n",
    "        \"dropout_rate\": self.dropout_rate\n",
    "    })\n",
    "    return config\n",
    "  \n",
    "  @classmethod\n",
    "  def from_config(cls, config):\n",
    "    # Extract required arguments from config\n",
    "    num_layers = config.pop('num_layers')\n",
    "    d_model = config.pop('d_model')\n",
    "    num_heads = config.pop('num_heads')\n",
    "    dff = config.pop('dff')\n",
    "    input_vocab_size = config.pop('input_vocab_size')\n",
    "    target_vocab_size = config.pop('target_vocab_size')\n",
    "    dropout_rate = config.pop('dropout_rate', 0.1)  # Default value if not provided\n",
    "\n",
    "    # Create an instance of Transformer with extracted arguments\n",
    "    return cls(\n",
    "      num_layers=num_layers, \n",
    "      d_model=d_model, \n",
    "      num_heads=num_heads,\n",
    "      dff=dff, \n",
    "      input_vocab_size=input_vocab_size,\n",
    "      target_vocab_size=target_vocab_size, \n",
    "      dropout_rate=dropout_rate, \n",
    "      **config\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 4\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    dff=dff,\n",
    "    input_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n",
    "    dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = transformer((ques, sql))\n",
    "\n",
    "# print(ques.shape)\n",
    "# print(sql.shape)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "  \n",
    "  def get_config(self):\n",
    "        return {\n",
    "            'd_model': float(self.d_model),\n",
    "            'warmup_steps': self.warmup_steps\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "  mask = label != 0\n",
    "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "  loss = loss_object(label, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss *= mask\n",
    "\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
    "  return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  match = label == pred\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    loss=masked_loss,\n",
    "    optimizer=optimizer,\n",
    "    metrics=[masked_accuracy, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./model-state/best_model.keras',  # Path to save the model file\n",
    "    monitor='masked_accuracy',  # Monitor validation loss\n",
    "    save_best_only=True,  # Save only the best model \n",
    "    # save_weights_only=True,  # Save only the model weights\n",
    "    verbose=1  # Print verbose messages\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.3824 - loss: 0.8198 - masked_accuracy: 0.8370\n",
      "Epoch 1: masked_accuracy improved from -inf to 0.85320, saving model to ./model-state/best_model.keras\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3192s\u001b[0m 3s/step - accuracy: 0.3824 - loss: 0.8197 - masked_accuracy: 0.8370 - val_accuracy: 0.4093 - val_loss: 0.5902 - val_masked_accuracy: 0.8812\n",
      "Epoch 2/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4058 - loss: 0.4746 - masked_accuracy: 0.8978\n",
      "Epoch 2: masked_accuracy improved from 0.85320 to 0.90594, saving model to ./model-state/best_model.keras\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3360s\u001b[0m 3s/step - accuracy: 0.4058 - loss: 0.4745 - masked_accuracy: 0.8978 - val_accuracy: 0.4194 - val_loss: 0.3806 - val_masked_accuracy: 0.9221\n",
      "Epoch 3/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4239 - loss: 0.3206 - masked_accuracy: 0.9292\n",
      "Epoch 3: masked_accuracy improved from 0.90594 to 0.93363, saving model to ./model-state/best_model.keras\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3366s\u001b[0m 3s/step - accuracy: 0.4239 - loss: 0.3206 - masked_accuracy: 0.9292 - val_accuracy: 0.4316 - val_loss: 0.3120 - val_masked_accuracy: 0.9356\n",
      "Epoch 4/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4312 - loss: 0.2306 - masked_accuracy: 0.9471\n",
      "Epoch 4: masked_accuracy improved from 0.93363 to 0.94873, saving model to ./model-state/best_model.keras\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3364s\u001b[0m 3s/step - accuracy: 0.4312 - loss: 0.2306 - masked_accuracy: 0.9471 - val_accuracy: 0.4260 - val_loss: 0.2637 - val_masked_accuracy: 0.9443\n",
      "Epoch 5/5\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - accuracy: 0.4331 - loss: 0.1853 - masked_accuracy: 0.9564\n",
      "Epoch 5: masked_accuracy improved from 0.94873 to 0.95741, saving model to ./model-state/best_model.keras\n",
      "\u001b[1m1009/1009\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3178s\u001b[0m 3s/step - accuracy: 0.4331 - loss: 0.1853 - masked_accuracy: 0.9564 - val_accuracy: 0.4329 - val_loss: 0.2205 - val_masked_accuracy: 0.9526\n"
     ]
    }
   ],
   "source": [
    "history = transformer.fit(train_batches,\n",
    "    epochs=5,\n",
    "    validation_data=val_batches, \n",
    "    callbacks=[checkpoint_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.save('./model-state/full-epoch-2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'global_self_attention_20' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'encoder_layer_20' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'causal_self_attention_20' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'decoder_layer_20' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "path = './model-state/full-epoch-2.keras'\n",
    "new_model = tf.keras.models.load_model(\n",
    "    path, \n",
    "    custom_objects={\n",
    "        'Transformer': Transformer,\n",
    "        'CustomSchedule': CustomSchedule,\n",
    "        'masked_loss': masked_loss,\n",
    "        'masked_accuracy': masked_accuracy,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Transformer name=transformer, built=True>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "epoch = '(3,7)'\n",
    "\n",
    "file_path = './model-state/historys.pkl'\n",
    "tempH = history.history\n",
    "historys = []\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            historys = pickle.load(f)\n",
    "    except (pickle.UnpicklingError, EOFError, TypeError):\n",
    "        print(f\"Error: Unable to unpickle file '{file_path}'. It may be corrupted or not in a valid pickle format.\")\n",
    "\n",
    "tempH['epoch'] = epoch\n",
    "historys.append(tempH)\n",
    "\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(historys, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "  def __init__(self, tokenizers, transformer):\n",
    "    self.tokenizers = tokenizers\n",
    "    self.transformer = transformer\n",
    "\n",
    "  def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "    # The input sentence is Portuguese, hence adding the `[START]` and `[END]` tokens.\n",
    "    assert isinstance(sentence, tf.Tensor)\n",
    "    if len(sentence.shape) == 0:\n",
    "      sentence = sentence[tf.newaxis]\n",
    "\n",
    "    sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n",
    "\n",
    "    encoder_input = sentence\n",
    "\n",
    "    # As the output language is English, initialize the output with the\n",
    "    # English `[START]` token.\n",
    "    start_end = self.tokenizers.en.tokenize([''])[0]\n",
    "    start = start_end[0][tf.newaxis]\n",
    "    end = start_end[1][tf.newaxis]\n",
    "\n",
    "    # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
    "    # dynamic-loop can be traced by `tf.function`.\n",
    "    output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "    output_array = output_array.write(0, start)\n",
    "\n",
    "    for i in tf.range(max_length):\n",
    "      output = tf.transpose(output_array.stack())\n",
    "      predictions = self.transformer([encoder_input, output], training=False)\n",
    "\n",
    "      # Select the last token from the `seq_len` dimension.\n",
    "      predictions = predictions[:, -1:, :]  # Shape `(batch_size, 1, vocab_size)`.\n",
    "\n",
    "      predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "      # Concatenate the `predicted_id` to the output which is given to the\n",
    "      # decoder as its input.\n",
    "      output_array = output_array.write(i+1, predicted_id[0])\n",
    "\n",
    "      if predicted_id == end:\n",
    "        break\n",
    "\n",
    "    output = tf.transpose(output_array.stack())\n",
    "    # The output shape is `(1, tokens)`.\n",
    "    text = tokenizers.en.detokenize(output)[0]  # Shape: `()`.\n",
    "\n",
    "    tokens = tokenizers.en.lookup(output)[0]\n",
    "\n",
    "    # `tf.function` prevents us from using the attention_weights that were\n",
    "    # calculated on the last iteration of the loop.\n",
    "    # So, recalculate them outside the loop.\n",
    "    self.transformer([encoder_input, output[:,:-1]], training=False)\n",
    "    attention_weights = self.transformer.decoder.last_attn_scores\n",
    "\n",
    "    return text, tokens, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(tokenizers, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_translation(sentence, tokens, ground_truth):\n",
    "  print(f'{\"Input:\":15s}: {sentence}')\n",
    "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
    "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many times is the fuel propulsion is cng? <col1> Order Year <col2> Manufacturer <col3> Model <col4> Fleet Series (Quantity) <col5> Powertrain (Engine Transmission) <col6> Fuel Propulsion\n",
      "SELECT COUNT Fleet Series (Quantity) FROM table WHERE Fuel Propulsion = CNG\n"
     ]
    }
   ],
   "source": [
    "print(data.question_header.iloc[4])\n",
    "print(data.sql.iloc[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'query' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'key' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'value' (of type EinsumDense) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'global_self_attention_20' (of type GlobalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'encoder_layer_20' (of type EncoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'causal_self_attention_20' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/abdul-rehman/DISK-W/python/work-env/myvnv/lib/python3.10/site-packages/keras/src/layers/layer.py:847: UserWarning: Layer 'decoder_layer_20' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : What is the format for South Australia? State territory Text background colour Format Current slogan Current series Notes\n",
      "Prediction     : select avg paris e / s / league from table where league of e ' s e ' sen & e ' sen & e ' s ) = 6 and position = 6 and gen\n",
      "Ground truth   : SELECT Format FROM table WHERE State/territory = South Australia\n"
     ]
    }
   ],
   "source": [
    "sentence = 'What is the format for South Australia? State territory Text background colour Format Current slogan Current series Notes'\n",
    "ground_truth = 'SELECT Format FROM table WHERE State/territory = South Australia'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Input': 'What is the format for South Australia? State territory Text background colour Format Current slogan Current series Notes',\n",
       "  'Prediction': 'select lo ro ro from table where tddd = v and fm 7 and fm 2 = r',\n",
       "  'Ground truth': 'SELECT Format FROM table WHERE State/territory = South Australia',\n",
       "  'epoch': '(1,1)'},\n",
       " {'Input': 'What is the format for South Australia? State territory Text background colour Format Current slogan Current series Notes',\n",
       "  'Prediction': \"select avg paris e / s / league from table where league of e ' s e ' sen & e ' sen & e ' s ) = 6 and position = 6 and gen\",\n",
       "  'Ground truth': 'SELECT Format FROM table WHERE State/territory = South Australia',\n",
       "  'epoch': '(2,2)'}]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "epoch = '(,)'\n",
    "\n",
    "file_path = './model-state/predictions.pkl'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            predictions = pickle.load(f)\n",
    "    except (pickle.UnpicklingError, EOFError, TypeError):\n",
    "        print(f\"Error: Unable to unpickle file '{file_path}'. It may be corrupted or not in a valid pickle format.\")\n",
    "\n",
    "predictions.append({\n",
    "    'epoch': epoch,\n",
    "    'Input': sentence,\n",
    "    'Prediction': translated_text,\n",
    "    'Ground truth': ground_truth\n",
    "\n",
    "})\n",
    "# with open(file_path, 'wb') as f:\n",
    "#     pickle.dump(predictions, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:         : Name the background colour for the Australian Capital Territory State territory Text background colour Format Current slogan Current series Notes\n",
      "Prediction     : select en market from table where otherman = gamesress\n",
      "Ground truth   : SELECT Text/background colour FROM table WHERE State/territory = Australian Capital Territory\n"
     ]
    }
   ],
   "source": [
    "sentence='Name the background colour for the Australian Capital Territory State territory Text background colour Format Current slogan Current series Notes'\n",
    "ground_truth='SELECT Text/background colour FROM table WHERE State/territory = Australian Capital Territory'\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhow many times is the fuel propulsion is cng? Order Year Manufacturer Model Fleet Series (Quantity) Powertrain (Engine Transmission) Fuel Propulsion\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      2\u001b[0m ground_truth\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT COUNT Fleet Series (Quantity) FROM table WHERE Fuel Propulsion = CNG\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m translated_text, translated_tokens, attention_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtranslator\u001b[49m(\n\u001b[1;32m      5\u001b[0m     tf\u001b[38;5;241m.\u001b[39mconstant(sentence))\n\u001b[1;32m      6\u001b[0m print_translation(sentence, translated_text, ground_truth)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translator' is not defined"
     ]
    }
   ],
   "source": [
    "sentence= 'how many times is the fuel propulsion is cng? Order Year Manufacturer Model Fleet Series (Quantity) Powertrain (Engine Transmission) Fuel Propulsion'\n",
    "ground_truth='SELECT COUNT Fleet Series (Quantity) FROM table WHERE Fuel Propulsion = CNG'\n",
    "\n",
    "translated_text, translated_tokens, attention_weights = translator(\n",
    "    tf.constant(sentence))\n",
    "print_translation(sentence, translated_text, ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
